{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ryukijano/Pytorch_3d_ryu/blob/main/bundle_adjustment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bD6DUkgzmFoR"
      },
      "outputs": [],
      "source": [
        "# Copyright (c) Meta Platforms, Inc. and affiliates. All rights reserved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj6j6__ZmFoW"
      },
      "source": [
        "# Absolute camera orientation given set of relative camera pairs\n",
        "\n",
        "This tutorial showcases the `cameras`, `transforms` and `so3` API.\n",
        "\n",
        "The problem we deal with is defined as follows:\n",
        "\n",
        "Given an optical system of $N$ cameras with extrinsics $\\{g_1, ..., g_N | g_i \\in SE(3)\\}$, and a set of relative camera positions $\\{g_{ij} | g_{ij}\\in SE(3)\\}$ that map between coordinate frames of randomly selected pairs of cameras $(i, j)$, we search for the absolute extrinsic parameters $\\{g_1, ..., g_N\\}$ that are consistent with the relative camera motions.\n",
        "\n",
        "More formally:\n",
        "$$\n",
        "g_1, ..., g_N = \n",
        "{\\arg \\min}_{g_1, ..., g_N} \\sum_{g_{ij}} d(g_{ij}, g_i^{-1} g_j),\n",
        "$$,\n",
        "where $d(g_i, g_j)$ is a suitable metric that compares the extrinsics of cameras $g_i$ and $g_j$. \n",
        "\n",
        "Visually, the problem can be described as follows. The picture below depicts the situation at the beginning of our optimization. The ground truth cameras are plotted in purple while the randomly initialized estimated cameras are plotted in orange:\n",
        "![Initialization](https://github.com/facebookresearch/pytorch3d/blob/main/docs/tutorials/data/bundle_adjustment_initialization.png?raw=1)\n",
        "\n",
        "Our optimization seeks to align the estimated (orange) cameras with the ground truth (purple) cameras, by minimizing the discrepancies between pairs of relative cameras. Thus, the solution to the problem should look as follows:\n",
        "![Solution](https://github.com/facebookresearch/pytorch3d/blob/main/docs/tutorials/data/bundle_adjustment_final.png?raw=1)\n",
        "\n",
        "In practice, the camera extrinsics $g_{ij}$ and $g_i$ are represented using objects from the `SfMPerspectiveCameras` class initialized with the corresponding rotation and translation matrices `R_absolute` and `T_absolute` that define the extrinsic parameters $g = (R, T); R \\in SO(3); T \\in \\mathbb{R}^3$. In order to ensure that `R_absolute` is a valid rotation matrix, we represent it using an exponential map (implemented with `so3_exp_map`) of the axis-angle representation of the rotation `log_R_absolute`.\n",
        "\n",
        "Note that the solution to this problem could only be recovered up to an unknown global rigid transformation $g_{glob} \\in SE(3)$. Thus, for simplicity, we assume knowledge of the absolute extrinsics of the first camera $g_0$. We set $g_0$ as a trivial camera $g_0 = (I, \\vec{0})$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAQY4EnHmFoX"
      },
      "source": [
        "## 0. Install and Import Modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAHR1LMJmP-h"
      },
      "source": [
        "Ensure `torch` and `torchvision` are installed. If `pytorch3d` is not installed, install it using the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uo7a3gdImMZx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64d8016d-d11f-45f6-acec-c2b5bba842f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting iopath\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from fvcore) (1.22.4)\n",
            "Collecting yacs>=0.1.6\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from fvcore) (6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from fvcore) (4.65.0)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.9/dist-packages (from fvcore) (2.2.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.9/dist-packages (from fvcore) (8.4.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.9/dist-packages (from fvcore) (0.8.10)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.9/dist-packages (from iopath) (4.5.0)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61429 sha256=d4b94cb7880332deb58f8cb949970737b11ff72fda5281be7bdc69ab77d04d48\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/42/02/66178d16e5c44dc26d309931834956baeda371956e86fbd876\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31547 sha256=98a2d39f37db909837c03172edfa7c7e06ac270a3c022252502b28247aaee752\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/13/6d/441d8f2af76ee6d2a3e67eebb1d0c556fefcee0a8b32266a8e\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, iopath, fvcore\n",
            "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-2.7.0 yacs-0.1.8\n",
            "Looking in links: https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py39_cu118_pyt200/download.html\n",
            "Collecting pytorch3d\n",
            "  Downloading https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py39_cu118_pyt200/pytorch3d-0.7.3-cp39-cp39-linux_x86_64.whl (20.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m169.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fvcore in /usr/local/lib/python3.9/dist-packages (from pytorch3d) (0.1.5.post20221221)\n",
            "Requirement already satisfied: iopath in /usr/local/lib/python3.9/dist-packages (from pytorch3d) (0.1.10)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.9/dist-packages (from fvcore->pytorch3d) (0.8.10)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from fvcore->pytorch3d) (6.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.9/dist-packages (from fvcore->pytorch3d) (8.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.9/dist-packages (from fvcore->pytorch3d) (2.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from fvcore->pytorch3d) (1.22.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from fvcore->pytorch3d) (4.65.0)\n",
            "Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.9/dist-packages (from fvcore->pytorch3d) (0.1.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from iopath->pytorch3d) (4.5.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.9/dist-packages (from iopath->pytorch3d) (2.7.0)\n",
            "Installing collected packages: pytorch3d\n",
            "Successfully installed pytorch3d-0.7.3\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "need_pytorch3d=False\n",
        "try:\n",
        "    import pytorch3d\n",
        "except ModuleNotFoundError:\n",
        "    need_pytorch3d=True\n",
        "if need_pytorch3d:\n",
        "    if torch.__version__.startswith((\"1.13.\", \"2.0.\")) and sys.platform.startswith(\"linux\"):\n",
        "        # We try to install PyTorch3D via a released wheel.\n",
        "        pyt_version_str=torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n",
        "        version_str=\"\".join([\n",
        "            f\"py3{sys.version_info.minor}_cu\",\n",
        "            torch.version.cuda.replace(\".\",\"\"),\n",
        "            f\"_pyt{pyt_version_str}\"\n",
        "        ])\n",
        "        !pip install fvcore iopath\n",
        "        !pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html\n",
        "    else:\n",
        "        # We try to install PyTorch3D from source.\n",
        "        !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgLa7XQimFoY"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import torch\n",
        "from pytorch3d.transforms.so3 import (\n",
        "    so3_exp_map,\n",
        "    so3_relative_angle,\n",
        ")\n",
        "from pytorch3d.renderer.cameras import (\n",
        "    SfMPerspectiveCameras,\n",
        ")\n",
        "\n",
        "# add path for demo utils\n",
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.abspath(''))\n",
        "\n",
        "# set for reproducibility\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"WARNING: CPU only, this will be slow!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4emnRuzmpRB"
      },
      "source": [
        "If using **Google Colab**, fetch the utils file for plotting the camera scene, and the ground truth camera positions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOvMPYJdmd15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03496074-e654-48d5-aa83-846a7f425683"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-14 17:32:24--  https://raw.githubusercontent.com/facebookresearch/pytorch3d/main/docs/tutorials/utils/camera_visualization.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2029 (2.0K) [text/plain]\n",
            "Saving to: ‘camera_visualization.py’\n",
            "\n",
            "camera_visualizatio 100%[===================>]   1.98K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-04-14 17:32:24 (33.8 MB/s) - ‘camera_visualization.py’ saved [2029/2029]\n",
            "\n",
            "--2023-04-14 17:32:25--  https://raw.githubusercontent.com/facebookresearch/pytorch3d/main/docs/tutorials/data/camera_graph.pth\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16896 (16K) [application/octet-stream]\n",
            "Saving to: ‘data/camera_graph.pth’\n",
            "\n",
            "camera_graph.pth    100%[===================>]  16.50K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2023-04-14 17:32:25 (8.89 MB/s) - ‘data/camera_graph.pth’ saved [16896/16896]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/facebookresearch/pytorch3d/main/docs/tutorials/utils/camera_visualization.py\n",
        "from camera_visualization import plot_camera_scene\n",
        "\n",
        "!mkdir data\n",
        "!wget -P data https://raw.githubusercontent.com/facebookresearch/pytorch3d/main/docs/tutorials/data/camera_graph.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9WD5vaimw3K"
      },
      "source": [
        "OR if running **locally** uncomment and run the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucGlQj5EmmJ5"
      },
      "outputs": [],
      "source": [
        "# from utils import plot_camera_scene"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WeEi7IgmFoc"
      },
      "source": [
        "## 1. Set up Cameras and load ground truth positions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n"
      ],
      "metadata": {
        "id": "56GwSnP-1jYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_Wm0zikmFod"
      },
      "outputs": [],
      "source": [
        "# load the SE3 graph of relative/absolute camera positions\n",
        "camera_graph_file = './data/camera_graph.pth'\n",
        "(R_absolute_gt, T_absolute_gt), \\\n",
        "    (R_relative, T_relative), \\\n",
        "    relative_edges = \\\n",
        "        torch.load(camera_graph_file)\n",
        "\n",
        "# create the relative cameras\n",
        "cameras_relative = SfMPerspectiveCameras(\n",
        "    R = R_relative.to(device),\n",
        "    T = T_relative.to(device),\n",
        "    device = device,\n",
        ")\n",
        "\n",
        "# create the absolute ground truth cameras\n",
        "cameras_absolute_gt = SfMPerspectiveCameras(\n",
        "    R = R_absolute_gt.to(device),\n",
        "    T = T_absolute_gt.to(device),\n",
        "    device = device,\n",
        ")\n",
        "\n",
        "# the number of absolute camera positions\n",
        "N = R_absolute_gt.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-f-RNlGemFog"
      },
      "source": [
        "## 2. Define optimization functions\n",
        "\n",
        "### Relative cameras and camera distance\n",
        "We now define two functions crucial for the optimization.\n",
        "\n",
        "**`calc_camera_distance`** compares a pair of cameras. This function is important as it defines the loss that we are minimizing. The method utilizes the `so3_relative_angle` function from the SO3 API.\n",
        "\n",
        "**`get_relative_camera`** computes the parameters of a relative camera that maps between a pair of absolute cameras. Here we utilize the `compose` and `inverse` class methods from the PyTorch3D Transforms API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzzk88RHmFoh"
      },
      "outputs": [],
      "source": [
        "def calc_camera_distance(cam_1, cam_2):\n",
        "    \"\"\"\n",
        "    Calculates the divergence of a batch of pairs of cameras cam_1, cam_2.\n",
        "    The distance is composed of the cosine of the relative angle between \n",
        "    the rotation components of the camera extrinsics and the l2 distance\n",
        "    between the translation vectors.\n",
        "    \"\"\"\n",
        "    # rotation distance\n",
        "    R_distance = (1.-so3_relative_angle(cam_1.R, cam_2.R, cos_angle=True)).mean()\n",
        "    # translation distance\n",
        "    T_distance = ((cam_1.T - cam_2.T)**2).sum(1).mean()\n",
        "    # the final distance is the sum\n",
        "    return R_distance + T_distance\n",
        "\n",
        "def get_relative_camera(cams, edges):\n",
        "    \"\"\"\n",
        "    For each pair of indices (i,j) in \"edges\" generate a camera\n",
        "    that maps from the coordinates of the camera cams[i] to \n",
        "    the coordinates of the camera cams[j]\n",
        "    \"\"\"\n",
        "\n",
        "    # first generate the world-to-view Transform3d objects of each \n",
        "    # camera pair (i, j) according to the edges argument\n",
        "    trans_i, trans_j = [\n",
        "        SfMPerspectiveCameras(\n",
        "            R = cams.R[edges[:, i]],\n",
        "            T = cams.T[edges[:, i]],\n",
        "            device = device,\n",
        "        ).get_world_to_view_transform()\n",
        "         for i in (0, 1)\n",
        "    ]\n",
        "    \n",
        "    # compose the relative transformation as g_i^{-1} g_j\n",
        "    trans_rel = trans_i.inverse().compose(trans_j)\n",
        "    \n",
        "    # generate a camera from the relative transform\n",
        "    matrix_rel = trans_rel.get_matrix()\n",
        "    cams_relative = SfMPerspectiveCameras(\n",
        "                        R = matrix_rel[:, :3, :3],\n",
        "                        T = matrix_rel[:, 3, :3],\n",
        "                        device = device,\n",
        "                    )\n",
        "    return cams_relative"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys9J7MbMmFol"
      },
      "source": [
        "## 3. Optimization\n",
        "Finally, we start the optimization of the absolute cameras.\n",
        "\n",
        "We use SGD with momentum and optimize over `log_R_absolute` and `T_absolute`. \n",
        "\n",
        "As mentioned earlier, `log_R_absolute` is the axis angle representation of the rotation part of our absolute cameras. We can obtain the 3x3 rotation matrix `R_absolute` that corresponds to `log_R_absolute` with:\n",
        "\n",
        "`R_absolute = so3_exp_map(log_R_absolute)`\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhpNFYR41VPk",
        "outputId": "58cf1154-a95a-47e8-8972-bb6f4b1c708e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (3.7.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (4.39.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (23.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (8.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (5.12.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.22.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib) (1.0.7)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOK_DUzVmFom",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "5d9e70df-85cc-4cd2-a591-5f47ac481edc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-32ab0bf44a42>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mit\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'iteration=%3d; camera_distance=%1.3e'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcamera_distance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mplot_camera_scene\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcameras_absolute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcameras_absolute_gt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Optimization finished.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/camera_visualization.py\u001b[0m in \u001b[0;36mplot_camera_scene\u001b[0;34m(cameras, cameras_gt, status)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \"\"\"\n\u001b[1;32m     35\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"3d\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: gca() got an unexpected keyword argument 'projection'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# initialize the absolute log-rotations/translations with random entries\n",
        "log_R_absolute_init = torch.randn(N, 3, dtype=torch.float32, device=device)\n",
        "T_absolute_init = torch.randn(N, 3, dtype=torch.float32, device=device)\n",
        "\n",
        "# furthermore, we know that the first camera is a trivial one \n",
        "#    (see the description above)\n",
        "log_R_absolute_init[0, :] = 0.\n",
        "T_absolute_init[0, :] = 0.\n",
        "\n",
        "# instantiate a copy of the initialization of log_R / T\n",
        "log_R_absolute = log_R_absolute_init.clone().detach()\n",
        "log_R_absolute.requires_grad = True\n",
        "T_absolute = T_absolute_init.clone().detach()\n",
        "T_absolute.requires_grad = True\n",
        "\n",
        "# the mask the specifies which cameras are going to be optimized\n",
        "#     (since we know the first camera is already correct, \n",
        "#      we only optimize over the 2nd-to-last cameras)\n",
        "camera_mask = torch.ones(N, 1, dtype=torch.float32, device=device)\n",
        "camera_mask[0] = 0.\n",
        "\n",
        "# init the optimizer\n",
        "optimizer = torch.optim.SGD([log_R_absolute, T_absolute], lr=.1, momentum=0.9)\n",
        "\n",
        "# run the optimization\n",
        "n_iter = 2000  # fix the number of iterations\n",
        "for it in range(n_iter):\n",
        "    # re-init the optimizer gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # compute the absolute camera rotations as \n",
        "    # an exponential map of the logarithms (=axis-angles)\n",
        "    # of the absolute rotations\n",
        "    R_absolute = so3_exp_map(log_R_absolute * camera_mask)\n",
        "\n",
        "    # get the current absolute cameras\n",
        "    cameras_absolute = SfMPerspectiveCameras(\n",
        "        R = R_absolute,\n",
        "        T = T_absolute * camera_mask,\n",
        "        device = device,\n",
        "    )\n",
        "\n",
        "    # compute the relative cameras as a composition of the absolute cameras\n",
        "    cameras_relative_composed = \\\n",
        "        get_relative_camera(cameras_absolute, relative_edges)\n",
        "\n",
        "    # compare the composed cameras with the ground truth relative cameras\n",
        "    # camera_distance corresponds to $d$ from the description\n",
        "    camera_distance = \\\n",
        "        calc_camera_distance(cameras_relative_composed, cameras_relative)\n",
        "\n",
        "    # our loss function is the camera_distance\n",
        "    camera_distance.backward()\n",
        "    \n",
        "    # apply the gradients\n",
        "    optimizer.step()\n",
        "\n",
        "    # plot and print status message\n",
        "    if it % 200==0 or it==n_iter-1:\n",
        "        status = 'iteration=%3d; camera_distance=%1.3e' % (it, camera_distance)\n",
        "        plot_camera_scene(cameras_absolute, cameras_absolute_gt, status)\n",
        "\n",
        "print('Optimization finished.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vncLMvxWnhmO"
      },
      "source": [
        "## 4. Conclusion \n",
        "\n",
        "In this tutorial we learnt how to initialize a batch of SfM Cameras, set up loss functions for bundle adjustment, and run an optimization loop. "
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jymXULTT1vRL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "bento_stylesheets": {
      "bento/extensions/flow/main.css": true,
      "bento/extensions/kernel_selector/main.css": true,
      "bento/extensions/kernel_ui/main.css": true,
      "bento/extensions/new_kernel/main.css": true,
      "bento/extensions/system_usage/main.css": true,
      "bento/extensions/theme/main.css": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "file_extension": ".py",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5+"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3
  },
  "nbformat": 4,
  "nbformat_minor": 0
}